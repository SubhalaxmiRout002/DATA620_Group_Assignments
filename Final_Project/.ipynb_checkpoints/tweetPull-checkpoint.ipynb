{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUNY Data 620 - Web Analytics, Summer 2020  \n",
    "**Final Project: Twitter Pull**   \n",
    "**Prof:** Alain Ledon  \n",
    "**Members:** Misha Kollontai, Amber Ferger, Zach Alexander, Subhalaxmi Rout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import GetOldTweets3 as got\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "We'll define the following functions:\n",
    "* **perdelta**: Based on a [stackoverflow](https://stackoverflow.com/questions/10688006/generate-a-list-of-datetimes-between-an-interval) thread, this will be used to generate a list of date ranges for our twitter pull. \n",
    "* **getTweets**: This will be used to pull the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ date function\n",
    "def perdelta(start, end, delta):\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta\n",
    "        \n",
    "################ get tweets function \n",
    "def getTweets(city, startDate, endDate):\n",
    "    n = 1000\n",
    "    \n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch('COVID')\\\n",
    "    .setSince(startDate)\\\n",
    "    .setUntil(endDate)\\\n",
    "    .setMaxTweets(n)\\\n",
    "    .setNear(city)\n",
    "    \n",
    "    ls = []\n",
    "    \n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    print(len(tweets))\n",
    "    for i in tweets:\n",
    "        ls.append([i.text, i.hashtags, city, startDate, endDate])\n",
    "    \n",
    "    return(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data\n",
    "\n",
    "#### Largest City by State\n",
    "\n",
    "* Read in a list of the top 1000 [cities]([https://public.opendatasoft.com/explore/dataset/1000-largest-us-cities-by-population-with-geographic-coordinates/table/?sort=-rank]) in the US\n",
    "* Select top city by state, extract geocoordinates\n",
    "* Split the geocoordinates data into 2 lists so that we can run on 2 separate machines (there is a max of 15 requests per 15 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cities doc, select top city from each \n",
    "# https://stackoverflow.com/questions/50415632/how-to-select-top-n-row-from-each-group-after-group-by-in-pandas\n",
    "allData = pd.read_csv('largeCities.csv', delimiter=';')\n",
    "final_cities = allData.sort_values(by = ['State', 'Population'], ascending=False).groupby(['State'], sort=False).head(1)\n",
    "coords = final_cities['Coordinates'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the coordinates into 2 lists\n",
    "mid = math.floor(len(coords)/2)\n",
    "coords1 = coords[0:mid]\n",
    "coords2 = coords[mid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Ranges\n",
    "Next, we'll generate date ranges for pull. Each range will represent 2 weeks, defined as Sunday - Saturday. The total span of the analysis will go from **3/8/2020** to **7/11/2020**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020-03-08', '2020-07-15')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_dates = []\n",
    "#for result in perdelta(datetime.date(2020, 3, 8), datetime.date(2020, 7, 6), datetime.timedelta(days=14)):  \n",
    "#    nextWk = result + datetime.timedelta(days=6)\n",
    "#    startDt = result.strftime(\"%Y-%m-%d\")\n",
    "#    endDt = nextWk.strftime(\"%Y-%m-%d\")   \n",
    "#    all_dates.append((startDt,endDt))\n",
    "    \n",
    "all_dates = [(datetime.date(2020, 3, 8).strftime(\"%Y-%m-%d\"), datetime.date(2020, 7, 15).strftime(\"%Y-%m-%d\"))]\n",
    "all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalList = []\n",
    "test = coords[0:2]\n",
    "\n",
    "for c in test:\n",
    "    print(c)\n",
    "    for d in all_dates:\n",
    "        ls = getTweets(c,d[0],d[1])\n",
    "        [finalList.append(x) for x in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(finalList, columns = ['TEXT', 'HASHTAGS', 'COORDS', 'WEEK_START', 'WEEK_END']) \n",
    "df.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.1399814,-104.8202462\n",
      "162\n",
      "43.0389025,-87.9064736\n",
      "1000\n",
      "38.3498195,-81.6326234\n",
      "747\n",
      "47.6062095,-122.3320708\n",
      "1000\n",
      "36.8529263,-75.977985\n",
      "1000\n",
      "44.4758825,-73.212072\n",
      "650\n",
      "40.7607793,-111.8910474\n",
      "1000\n",
      "29.7604267,-95.3698028\n",
      "1000\n",
      "35.1495343,-90.0489801\n",
      "1000\n",
      "43.5445959,-96.7311034\n",
      "661\n",
      "34.0007104,-81.0348144\n",
      "1000\n",
      "41.8239891,-71.4128343\n",
      "1000\n",
      "39.9525839,-75.1652215\n",
      "1000\n",
      "45.5230622,-122.6764816\n",
      "1000\n",
      "35.4675602,-97.5164276\n",
      "1000\n",
      "39.9611755,-82.9987942\n",
      "1000\n",
      "46.8771863,-96.7898034\n",
      "797\n",
      "35.2270869,-80.8431267\n",
      "1000\n",
      "40.7127837,-74.0059413\n",
      "1000\n",
      "35.0853336,-106.6055534\n",
      "1000\n",
      "40.735657,-74.1723667\n",
      "1000\n",
      "42.9956397,-71.4547891\n",
      "664\n",
      "36.1699412,-115.1398296\n",
      "1000\n",
      "41.2523634,-95.9979883\n",
      "1000\n",
      "45.7832856,-108.5006904\n",
      "213\n"
     ]
    }
   ],
   "source": [
    "# Cycle through all cities\n",
    "finalList = []\n",
    "\n",
    "for i,c in enumerate(coords1):\n",
    "    print(c)\n",
    "    if (i+1) % 15 == 0:\n",
    "        time.sleep(930) # wait 15 min, 30 sec before continuing\n",
    "    for d in all_dates:\n",
    "        ls = getTweets(c,d[0],d[1])\n",
    "        [finalList.append(x) for x in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(finalList, columns = ['TEXT', 'HASHTAGS', 'COORDS', 'WEEK_START', 'WEEK_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalCsv = pd.merge(left=df, right=final_cities, left_on='COORDS', right_on='Coordinates')\n",
    "finalCsv = finalCsv.drop(columns=['Rank', 'Growth From 2000 to 2013', 'Coordinates'])\n",
    "finalCsv.to_csv('csv1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
