{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUNY Data 620 - Web Analytics, Summer 2020  \n",
    "**Final Project**   \n",
    "**Prof:** Alain Ledon  \n",
    "**Members:** Misha Kollontai, Amber Ferger, Zach Alexander, Subhalaxmi Rout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Your project should incorporate one or both of the two main themes of this course: network analysis and text processing. You need to show all of your work in a coherent workflow, and in a reproducible format, such as an IPython Notebook or an R Markdown document. If you are building a model or models, explain how you evaluate the “goodness” of the chosen model and parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Questions\n",
    "\n",
    "* Is there a relationship between location-specific Covid-19 sentiment and the number of positive cases within that region? \n",
    "* Does positive sentiment preceed spikes in positive cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "We will be using the Twitter API to scrape Tweet data, [John's Hopkins COVID-19 Data](https://github.com/CSSEGISandData/COVID-19) and [Wikipedia](https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States) for the COVID-19 numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Plan\n",
    "\n",
    "1. Scrape Twitter data from 2 locations - perhaps NYC (severe initial wave) and New Orleans (experiencing something of a second wave)\n",
    "2. Pull coronavirus case numbers for the 2 locations in question\n",
    "3. Perform sentiment analyis on the tweets collected and aggregate them into an overall sentiment index for each day\n",
    "4. Plot timeseries of the sentiment index -vs- Coronavirus case numbers\n",
    "5. Indicate important moments on the timeline related to Covid-19 safety measures or announcements\n",
    "6. Investigate potential relationships between the two sets and compare the relationships from one city to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### sentiment analysis\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    neg = score['neg']\n",
    "    pos = score['pos']\n",
    "    neu = score['neu']\n",
    "    compound= score['compound']\n",
    "    \n",
    "    return [neg,pos,neu,compound, sentence]\n",
    "\n",
    "############### splitting hashtag groupings\n",
    "def splitTags(x,y):\n",
    "    return [(x,z) for z in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "**TO DO: Explanation of twitter data pull**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and replace nulls\n",
    "tweets = pd.read_csv('Covid_Twitter_City_Data.csv', delimiter=',')\n",
    "tweets = tweets.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate sentiment analyzer, define function for sentiment output\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# sentiments to dataframe\n",
    "text = tweets['TEXT'].tolist()\n",
    "sentiments = [sentiment_analyzer_scores(s) for s in text]\n",
    "sentiments_df = pd.DataFrame(sentiments, columns = ['NEGATIVE_SCORE', 'POSITIVE_SCORE', 'NEUTRAL_SCORE', 'COMPOUND', 'SENTENCE'])\n",
    "\n",
    "# final dataframe with sentiments\n",
    "finalFrame = tweets.join(sentiments_df)\n",
    "finalFrame = finalFrame.iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Network\n",
    "* **Nodes**: Cities, **TO DO: color: sentiment, size: portion of population affected by covid**\n",
    "* **Edges**: Shared Hashtags, **TO DO: edge weights: number of shared hashtags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], ['#CX', '#COVID', '#custserv']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hashtags & coordinates for each record\n",
    "hashtags = tweets['HASHTAGS'].tolist()\n",
    "coords = tweets['COORDS'].tolist()\n",
    "sepHash = [i.split() for i in hashtags]\n",
    "sepHash[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example output from the hashtag #FollowTheScience:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['34.7464809,-92.2895948',\n",
       " '39.7392358,-104.990251',\n",
       " '47.6062095,-122.3320708',\n",
       " '34.0522342,-118.2436849',\n",
       " '39.9525839,-75.1652215',\n",
       " '41.8781136,-87.6297982',\n",
       " '38.9071923,-77.0368707',\n",
       " '39.9611755,-82.9987942']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set of all coordinates with individual hashtag\n",
    "coordTag = [splitTags(i,j) for i,j in list(zip(coords,sepHash)) if len(j)> 0]\n",
    "flattened = [val for sublist in coordTag for val in sublist]\n",
    "finalHash = set(flattened)\n",
    "\n",
    "# create a dictionary of each hashtag with the city coordinates\n",
    "tempDict = {}\n",
    "for i,j in finalHash:\n",
    "    if j not in tempDict:\n",
    "        tempDict[j]= [i]\n",
    "    else:\n",
    "        tempDict[j].append(i)\n",
    "        \n",
    "# remove covid hashtags from dictionary\n",
    "tagsToRemove = ['#covid_19', '#COVID19', '#COVID2019', '#COVID_19', '#COVID__19', '#COVID', '#COVD19', '#Covid_19']\n",
    "\n",
    "for k in tagsToRemove:\n",
    "    tempDict.pop(k, None)\n",
    "\n",
    "print('Example output from the hashtag #FollowTheScience:')\n",
    "tempDict['#FollowTheScience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Edge: ('47.6062095,-122.3320708', '45.5230622,-122.6764816')\n"
     ]
    }
   ],
   "source": [
    "#### Final Edges\n",
    "# combining all elements in the dictionary values into separate node connections\n",
    "coordPairs = list(tempDict.values())\n",
    "productList = []\n",
    "\n",
    "for i in coordPairs:\n",
    "    if len(i) >1:\n",
    "        productList.append(list(combinations(i,2)))\n",
    "    \n",
    "finalPairs = [val for sublist in productList for val in sublist]\n",
    "\n",
    "print('Example Edge:', finalPairs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Weights\n",
    "https://www.geeksforgeeks.org/python-program-to-count-duplicates-in-a-list-of-tuples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scraping Twitter Data from New York City & New Orleans\n",
    "\n",
    "As a first step, we decided to scrape tweets from two locations, New York City and New Orleans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the tweets from NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('covid_tweets.csv', delimiter='\\t')\n",
    "tweets['City'] = 'NYC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Pulling coronavirus case numbers for both locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_cases = pd.read_csv('confirmed_cases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering for NYC cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After locating the correct county FIPS number for New York City, we were able to filter the pandas dataframe to only include this row. Additionally, we transposed this row to ensure we had one column designated for the date and another for the number of confirmed cases for that corresponding date. Finally, we made sure to reset the index and adjust the date type in order to be able to show our visuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_filtered = covid_cases[covid_cases['FIPS'] == 36061]\n",
    "df = cases_filtered.iloc[:, 11:186:1]\n",
    "\n",
    "df = df.transpose().reset_index()\n",
    "df = df.rename(columns={'index': 'Date', 1863: \"Confirmed_Cases\"})\n",
    "\n",
    "nyc_time_series = pd.DataFrame(df, columns = ['Date','Confirmed_Cases'])\n",
    "nyc_time_series['Date'] = pd.to_datetime(nyc_time_series['Date'], format='%m/%d/%y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the filtered dataset with just NYC cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_time_series.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to find the number of new cases per day, we can utilize our confirmed cases column to take the difference between the current day and the previous day. Additionally, for our visualization, we can take the 7-day average of new cases and plot this as well, in order to obtain a better view of trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_newcases(df):\n",
    "    df['New_Cases'] = 'NA'\n",
    "    for i in range(0, len(df['Confirmed_Cases'])):\n",
    "        if i == 0:\n",
    "            df['New_Cases'][i] = 0\n",
    "        else:\n",
    "            df['New_Cases'][i] = df['Confirmed_Cases'][i] - df['Confirmed_Cases'][i-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sevenday(df):\n",
    "    df['Seven_Day_Avg'] = 'NA'\n",
    "    for i in range(0, len(df['Confirmed_Cases'])):\n",
    "        if i < 8:\n",
    "            df['Seven_Day_Avg'][i] = 0\n",
    "        else:\n",
    "            weekly = []\n",
    "            for y in range(0,7):\n",
    "                weekly.append(df['New_Cases'][i-y])\n",
    "            df['Seven_Day_Avg'][i] = sum(weekly) / 7\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_newcases(nyc_time_series)\n",
    "df = add_sevenday(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the `New Cases` and `Seven Day Average` columns, we can create a plot to show the case counts in New York City:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawNewCases(df, title, fignum, var):\n",
    "    var = plt.figure(fignum, figsize=(16,8))\n",
    "    plt.bar(df['Date'], df['New_Cases'], color='indianred', alpha=0.4)\n",
    "    plt.plot(df['Date'], df['Seven_Day_Avg'], c='indianred', linewidth=2)\n",
    "    plt.plot(legend=None)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Number of New Cases')\n",
    "    plt.gca().xaxis.set_major_formatter(fmt)\n",
    "    var.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = mdates.MonthLocator()\n",
    "fmt = mdates.DateFormatter('%B')\n",
    "\n",
    "nyc_time_series = pd.DataFrame(df, columns = ['Date','Confirmed_Cases', 'New_Cases', 'Seven_Day_Avg'])\n",
    "nyc_time_series['Date'] = pd.to_datetime(nyc_time_series['Date'], format='%m/%d/%y')\n",
    "\n",
    "drawNewCases(nyc_time_series, 'Number of new COVID-19 cases in New York City (Daily)', 1, 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering for New Orleans Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_filtered_newo = covid_cases[covid_cases['FIPS'] == 22071]\n",
    "\n",
    "\n",
    "df_newo = cases_filtered_newo.iloc[:, 11:186:1]\n",
    "\n",
    "df_newo = df_newo.transpose().reset_index()\n",
    "df_newo = df_newo.rename(columns={'index': 'Date', 1153: \"Confirmed_Cases\"})\n",
    "\n",
    "newo_time_series = pd.DataFrame(df_newo, columns = ['Date','Confirmed_Cases'])\n",
    "newo_time_series['Date'] = pd.to_datetime(newo_time_series['Date'], format='%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newo_time_series.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newo = add_newcases(newo_time_series)\n",
    "df_newo = add_sevenday(df_newo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newo_time_series = pd.DataFrame(df_newo, columns = ['Date','Confirmed_Cases', 'New_Cases', 'Seven_Day_Avg'])\n",
    "newo_time_series['Date'] = pd.to_datetime(newo_time_series['Date'], format='%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawNewCases(newo_time_series, 'Number of new COVID-19 cases in New Orleans (Daily)', 2, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note from Zach**: Will remove this commented-out code later (see below), but thought I'd leave it just in case it'll be helpful for future visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locator = mdates.MonthLocator()\n",
    "# fmt = mdates.DateFormatter('%B')\n",
    "\n",
    "\n",
    "# plt.plot(nyc_time_series['Date'], nyc_time_series['Confirmed_Cases'], c='indianred')\n",
    "# plt.plot(legend=None)\n",
    "# plt.title('Number of Confirmed COVID-19 Cases in New York City')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Number of Confirmed Cases')\n",
    "# plt.gca().xaxis.set_major_formatter(fmt)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
