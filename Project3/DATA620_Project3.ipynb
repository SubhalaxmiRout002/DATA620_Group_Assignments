{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUNY Data 620 - Web Analytics, Summer 2020  \n",
    "**Group Project 3**   \n",
    "**Prof:** Alain Ledon  \n",
    "**Members:** Misha Kollontai, Amber Ferger, Zach Alexander, Subhalaxmi Rout  \n",
    "  \n",
    "**YouTube Link**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python,\n",
    "and any features you can think of, build the best name gender classifier you can. \n",
    "\n",
    "Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the devtest set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set.\n",
    "\n",
    "\n",
    "How does the performance on the test set compare to the performance on the dev-test set? Is this what\n",
    "you'd expect? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import math\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The *names* corpus in the nltk package contains the names and genders of 7,944 individuals. First, we will compile a list of all names with their gender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2943 male names in the dataset.\n",
      "There are 5001 female names in the dataset.\n"
     ]
    }
   ],
   "source": [
    "males = [(name, 'male') for name in names.words('male.txt')]\n",
    "numMales = len(males)\n",
    "females = [(name, 'female') for name in names.words('female.txt')]\n",
    "numFemales = len(females)\n",
    "\n",
    "print(f'There are {numMales} male names in the dataset.')\n",
    "print(f'There are {numFemales} female names in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the lists and shuffle the data so that all names of the same gender are not together. We can confirm that the names are shuffled by looking at the genders of the first 5 individuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 names in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Cordelie', 'female'),\n",
       " ('Peggie', 'female'),\n",
       " ('Solange', 'female'),\n",
       " ('Rana', 'female'),\n",
       " ('Jessy', 'female')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "allNames = males + females\n",
    "random.shuffle(allNames)\n",
    "\n",
    "print('First 5 names in the dataset:')\n",
    "allNames[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Features\n",
    "Next, we'll define a function to create features for our names. The initial features will include:\n",
    "* **last_letter**: The last letter of the given name.\n",
    "* **first_letter**: The first letter of the given name. \n",
    "* **name_length**: The length of the given name.\n",
    "* **num_vowels**: The number of vowels in the given name.\n",
    "* **num_consonants**: The number of consonants in the given name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features['last_letter'] = name[-1]\n",
    "    features['first_letter'] = name[0]\n",
    "    features['name_length'] = len(name)  \n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    vowelLength = len([i for i in name if i in vowels])\n",
    "    features['num_vowels'] = vowelLength\n",
    "    features['num_consonants'] = len(name) - vowelLength\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split\n",
    "Now that we've defined our feature function, we can run it on our dataset and split it into training, testing, and dev testing sets. \n",
    "* **Training Set**: This data will be used to train our classifiers and fit the models.\n",
    "* **Dev Test Set**: This data will be used to predict the gender (male or female). It will provide an unbiased evaluation of a model fit on the training dataset. We can use the results of the development set to tune our model. \n",
    "* **Test Set**: This data will be used to compute the accuracy of the final model. Since the model has never seen this data, it will provide an unbiased evaluation of the clasifier.\n",
    "\n",
    "The splits will be in the format of ({features}, gender). We will store the names and genders of the individuals in separate lists for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num records - train set:  6944\n",
      "Num records - dev test set:  500\n",
      "Num records - test set:  500\n"
     ]
    }
   ],
   "source": [
    "def tts(featureFunc, nameList):\n",
    "    featureSet = [(featureFunc(n),g) for (n,g) in nameList]\n",
    "    test_set, devtest_set, train_set = featureSet[0:500], featureSet[500:1000], featureSet[1000:] \n",
    "    tsName = nameList[0:500]\n",
    "    dtName = nameList[500:1000]\n",
    "    tName = nameList[1000:]\n",
    "    \n",
    "    return test_set, devtest_set, train_set, tsName, dtName, tName\n",
    "\n",
    "test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features, allNames)\n",
    "\n",
    "print('Num records - train set: ', len(train_set))\n",
    "print('Num records - dev test set: ', len(devtest_set))\n",
    "print('Num records - test set: ', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Classifier - Naive Bayes Classifier\n",
    "Now that we've split our data into training, development, and test sets, we can create a **Naive Bayes Classifier** to predict the gender of the names. In this type of model, each feature gets a say in determining which label should be assigned to a given input value. The prior probability is calculated for each label (male, female), and the contribution from each feature is combined with this probability to arrive at a likelihood estimate for each label.\n",
    "\n",
    "We will measure the accuracy of the model (the percentage of names the classifier predicts correctly) using the development test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.782\n"
     ]
    }
   ],
   "source": [
    "nbClass = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: ', nltk.classify.accuracy(nbClass, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the most important features used for predicting the gender. For each feature, this tells us the ratio of occurences for each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     33.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     29.2 : 1.0\n",
      "             last_letter = 'p'              male : female =     18.6 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.2 : 1.0\n",
      "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.8 : 1.0\n",
      "             last_letter = 'm'              male : female =      9.2 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.0 : 1.0\n",
      "             last_letter = 'w'              male : female =      8.0 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.7 : 1.0\n",
      "            first_letter = 'w'              male : female =      4.6 : 1.0\n",
      "              num_vowels = 5              female : male   =      4.5 : 1.0\n",
      "             last_letter = 'b'              male : female =      4.4 : 1.0\n",
      "             last_letter = 's'              male : female =      4.3 : 1.0\n",
      "             last_letter = 'g'              male : female =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nbClass.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the last letter and number of vowels in the names appear to be the driving factors. \n",
    "\n",
    "We can also generate a list of errors to see which names we've classified improperly. This will help us identify what additional features we should add to make the classification more accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 109\n"
     ]
    }
   ],
   "source": [
    "def pred_calc(nameList, featureFunc, nbClass):\n",
    "    preds = []\n",
    "    errors = []\n",
    "    for (name,actual) in nameList:\n",
    "        guess = nbClass.classify(featureFunc(name))\n",
    "        preds.append((actual,guess,name))\n",
    "        if guess != actual:\n",
    "            errors.append((actual, guess, name))\n",
    "    \n",
    "    return preds, errors\n",
    "\n",
    "preds, errors = pred_calc(dtName, gender_features, nbClass)\n",
    "print('Number of errors:', len(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sort the errors by the last two characters of the first name, we can see that some combinations occur more frequently in males than females and vice versa. For example, the letters *ie* appear more often in male names and then letters *ly* appear more often in female names. Let's update our feature set to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('female', 'male', 'Em'),\n",
       " ('female', 'male', 'Talyah'),\n",
       " ('female', 'male', 'Shirah'),\n",
       " ('male', 'female', 'Donal'),\n",
       " ('female', 'male', 'Sam'),\n",
       " ('male', 'female', 'Fabian'),\n",
       " ('female', 'male', 'Sean'),\n",
       " ('male', 'female', 'Coleman'),\n",
       " ('male', 'female', 'Christian'),\n",
       " ('male', 'female', 'Adrian'),\n",
       " ('male', 'female', 'Vaughan'),\n",
       " ('female', 'male', 'Meggan'),\n",
       " ('female', 'male', 'Gay'),\n",
       " ('male', 'female', 'Murray'),\n",
       " ('male', 'female', 'Lawrence'),\n",
       " ('male', 'female', 'Bruce'),\n",
       " ('male', 'female', 'Lawerence'),\n",
       " ('male', 'female', 'Erich'),\n",
       " ('female', 'male', 'Dulcy'),\n",
       " ('male', 'female', 'Randi'),\n",
       " ('male', 'female', 'Lindy'),\n",
       " ('female', 'male', 'Freddy'),\n",
       " ('male', 'female', 'Jessee'),\n",
       " ('male', 'female', 'Mikel'),\n",
       " ('male', 'female', 'Nathaniel'),\n",
       " ('female', 'male', 'Pen'),\n",
       " ('female', 'male', 'Gwen'),\n",
       " ('female', 'male', 'Grier'),\n",
       " ('female', 'male', 'Delores'),\n",
       " ('female', 'male', 'Dew'),\n",
       " ('female', 'male', 'Sukey'),\n",
       " ('male', 'female', 'Carey'),\n",
       " ('female', 'male', 'Sophey'),\n",
       " ('female', 'male', 'Sibley'),\n",
       " ('female', 'male', 'Daffy'),\n",
       " ('male', 'female', 'Rutledge'),\n",
       " ('female', 'male', 'Margo'),\n",
       " ('female', 'male', 'Angy'),\n",
       " ('male', 'female', 'Jean-Christophe'),\n",
       " ('male', 'female', 'Sollie'),\n",
       " ('male', 'female', 'Christie'),\n",
       " ('male', 'female', 'Georgie'),\n",
       " ('male', 'female', 'Bobbie'),\n",
       " ('male', 'female', 'Kermie'),\n",
       " ('male', 'female', 'Dennie'),\n",
       " ('male', 'female', 'Pennie'),\n",
       " ('male', 'female', 'Maurie'),\n",
       " ('male', 'female', 'Rickie'),\n",
       " ('female', 'male', 'Charin'),\n",
       " ('female', 'male', 'Devin'),\n",
       " ('female', 'male', 'Cristin'),\n",
       " ('female', 'male', 'Shir'),\n",
       " ('female', 'male', 'Wandis'),\n",
       " ('female', 'male', 'Maris'),\n",
       " ('male', 'female', 'Thorndike'),\n",
       " ('male', 'female', 'Ricki'),\n",
       " ('female', 'male', 'Tomiko'),\n",
       " ('female', 'male', 'Vicky'),\n",
       " ('male', 'female', 'Gayle'),\n",
       " ('male', 'female', 'Yale'),\n",
       " ('male', 'female', 'Beale'),\n",
       " ('male', 'female', 'Pascale'),\n",
       " ('male', 'female', 'Emile'),\n",
       " ('female', 'male', 'Merrill'),\n",
       " ('female', 'male', 'Joly'),\n",
       " ('female', 'male', 'Hally'),\n",
       " ('female', 'male', 'Vally'),\n",
       " ('female', 'male', 'Holly'),\n",
       " ('male', 'female', 'Baily'),\n",
       " ('male', 'female', 'Guillaume'),\n",
       " ('male', 'female', 'Tome'),\n",
       " ('female', 'male', 'Clemmy'),\n",
       " ('male', 'female', 'Zane'),\n",
       " ('male', 'female', 'Simone'),\n",
       " ('male', 'female', 'Dani'),\n",
       " ('female', 'male', 'Jo-Ann'),\n",
       " ('female', 'male', 'Quinn'),\n",
       " ('female', 'male', 'Melicent'),\n",
       " ('female', 'male', 'Vonny'),\n",
       " ('female', 'male', 'Vinny'),\n",
       " ('female', 'male', 'Penny'),\n",
       " ('female', 'male', 'Rhianon'),\n",
       " ('female', 'male', 'Rhiamon'),\n",
       " ('male', 'female', 'Ferguson'),\n",
       " ('female', 'male', 'Leanor'),\n",
       " ('female', 'male', 'Charlot'),\n",
       " ('male', 'female', 'Eliot'),\n",
       " ('female', 'male', 'Harriot'),\n",
       " ('male', 'female', 'Phillipe'),\n",
       " ('male', 'female', 'Demetre'),\n",
       " ('male', 'female', 'Henri'),\n",
       " ('female', 'male', 'Merl'),\n",
       " ('female', 'male', 'Gert'),\n",
       " ('female', 'male', 'Dory'),\n",
       " ('female', 'male', 'Gerry'),\n",
       " ('male', 'female', 'Maurise'),\n",
       " ('female', 'male', 'Lindsy'),\n",
       " ('male', 'female', 'Juanita'),\n",
       " ('male', 'female', 'Durante'),\n",
       " ('female', 'male', 'Janith'),\n",
       " ('female', 'male', 'Marybeth'),\n",
       " ('female', 'male', 'Helen-Elizabeth'),\n",
       " ('male', 'female', 'Morty'),\n",
       " ('female', 'male', 'Fawn'),\n",
       " ('male', 'female', 'Saxe'),\n",
       " ('female', 'male', 'Jesselyn'),\n",
       " ('female', 'male', 'Marilyn'),\n",
       " ('female', 'male', 'Roselyn'),\n",
       " ('female', 'male', 'Ardys')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(errors, key=lambda x: x[-1][-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set Revamp\n",
    "\n",
    "**Last two letters**: First, let's add in a feature for the last 2 letters of each name. We'll recreate our train, test, and dev test splits and run the Naive Bayes Classifer on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.82\n",
      "Number of errors: 90\n"
     ]
    }
   ],
   "source": [
    "def gender_features2(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features['last_letter'] = name[-1]\n",
    "    features['first_letter'] = name[0]\n",
    "    features['name_length'] = len(name)    \n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    vowelLength = len([i for i in name if i in vowels])\n",
    "    features['num_vowels'] = vowelLength\n",
    "    features['num_consonants'] = len(name) - vowelLength\n",
    "    \n",
    "    # add in feature for last 2 letters of name\n",
    "    features['last_two_letters'] = name[-2:]\n",
    "    \n",
    "    # add in feature for first 2 letters of name\n",
    "    features['first_two_letters'] = name[:2]\n",
    "    \n",
    "    # presence of double letters:\n",
    "    def find_dbl_ltrs(x):\n",
    "        groups = groupby(name)\n",
    "        result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "        return (len([x[1] for x in result if x[1]>1]))\n",
    "    features['dbl_ltrs'] = find_dbl_ltrs(name)\n",
    "\n",
    "    return features\n",
    "\n",
    "test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features2, allNames)\n",
    "nbClass2 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: ', nltk.classify.accuracy(nbClass2, devtest_set))\n",
    "\n",
    "preds2, errors2 = pred_calc(dtName, gender_features2, nbClass2)\n",
    "print('Number of errors:', len(errors2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy went up to 82%! Let's try again with some additional features.\n",
    "\n",
    "**Bouba and Kiki Vowels/Consonants**: Sidhu and Pexman (1) discovered a relationship of Bouba with female first names and Kiki with male first names. We will use a modified version of their findings and define the following new features: \n",
    "* **num_bouba_cons**: Count of the letters *b*, *l*, *m*, and *n*. *(Female names tend to have more of these)*\n",
    "* **num_bouba_vowels**: Count of the letters *u* and *o*. *(Female names tend to have more of these)*\n",
    "* **num_kiki_cons**: Count of the letters *k*, *p*, and *t*. *(Male names tend to have more of these)*\n",
    "* **num_kiki_vowels**: Count of the letters *i* and *e*. *(Male names tend to have more of these)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.81\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/pdf/1606.05467.pdf\n",
    "\n",
    "def gender_features3(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features['last_letter'] = name[-1]\n",
    "    features['first_letter'] = name[0]\n",
    "    features['name_length'] = len(name)    \n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    vowelLength = len([i for i in name if i in vowels])\n",
    "    features['num_vowels'] = vowelLength\n",
    "    features['num_consonants'] = len(name) - vowelLength\n",
    "    \n",
    "    # add in feature for last 2 letters of name\n",
    "    features['last_two_letters'] = name[-2:]\n",
    "    \n",
    "    # add in feature for first 2 letters of name\n",
    "    features['first_two_letters'] = name[:2]\n",
    "    \n",
    "    # presence of double letters:\n",
    "    def find_dbl_ltrs(x):\n",
    "        groups = groupby(name)\n",
    "        result = [(label, sum(1 for _ in group)) for label, group in groups]\n",
    "        return (len([x[1] for x in result if x[1]>1]))\n",
    "    features['dbl_ltrs'] = find_dbl_ltrs(name)\n",
    "    \n",
    "    # add in bouba & kiki counts\n",
    "    boubaCons = ['b', 'l', 'm', 'n']\n",
    "    boubaVowels = ['u', 'o']\n",
    "    kikiCons = ['k', 'p', 't']\n",
    "    kikiVowels = ['i', 'e']\n",
    "    \n",
    "    bcLength = len([i for i in name if i in boubaCons])\n",
    "    bvLength = len([i for i in name if i in boubaVowels])\n",
    "    kcLength = len([i for i in name if i in kikiCons])\n",
    "    kvLength = len([i for i in name if i in kikiVowels])\n",
    "\n",
    "    features['num_bouba_cons'] = bcLength\n",
    "    features['num_bouba_vowels'] = bvLength\n",
    "    features['num_kiki_cons'] = kcLength\n",
    "    features['num_kiki_vowels'] = kvLength\n",
    "\n",
    "    return features\n",
    "\n",
    "test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features3, allNames)\n",
    "nbClass3 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: ', nltk.classify.accuracy(nbClass3, devtest_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 95\n"
     ]
    }
   ],
   "source": [
    "preds3, errors3 = pred_calc(dtName, gender_features3,nbClass3)\n",
    "print('Number of errors:', len(errors3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We can now evaluate the final model on our test set. First, we'll look at the overall accuracy of each of our subsequent models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>DEV_ACCURACY</th>\n",
       "      <th>TEST_ACCURACY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>First</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Second</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Final</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MODEL  DEV_ACCURACY  TEST_ACCURACY\n",
       "0   First         0.782          0.772\n",
       "1  Second         0.820          0.800\n",
       "2   Final         0.810          0.802"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([['First', nltk.classify.accuracy(nbClass, devtest_set), nltk.classify.accuracy(nbClass, test_set)], \n",
    "             ['Second', nltk.classify.accuracy(nbClass2, devtest_set), nltk.classify.accuracy(nbClass2, test_set)], \n",
    "             ['Final', nltk.classify.accuracy(nbClass3, devtest_set), nltk.classify.accuracy(nbClass3, test_set)]],\n",
    "            columns = ['MODEL', 'DEV_ACCURACY', 'TEST_ACCURACY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy on the development and test set increases from the first model to the final model. When looking at each model, we also notice that the accuracy on the test set is lower than on the development set. This is expected, as we tweaked our feature set based on the results of the development set and the test set contains data that the model has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPred, dtError = pred_calc(dtName, gender_features3,nbClass3)\n",
    "tsPred, tsError = pred_calc(tsName, gender_features3,nbClass3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Entropy Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label.\n",
    "\n",
    "Let's calculate the entropy of the labels for our dataset. Higher entropy implies better classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951030970454714\n"
     ]
    }
   ],
   "source": [
    "# make a list of male and female\n",
    "all_male_female = list(repeat('male', len(males))) + list(repeat('female', len(females)))\n",
    "def entropy(labels):    \n",
    "    freq_dist = nltk.FreqDist(labels)    \n",
    "    probs = [freq_dist.freq(i) for i in nltk.FreqDist(labels)]    \n",
    "    return -sum([j * math.log(j,2) for j in probs])\n",
    "\n",
    "print (entropy(all_male_female))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function shows that entropy is at 95%. <br> Let's create maximum entropy classifier model based on the features using training, deve_test, and test sets. We will apply the model with 3 different features i.e gender_features, gender_features2, and gender_features3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1, devtest_set_1, train_set_1, tsName_1, dtName_1, tName_1 = tts(gender_features, allNames)\n",
    "\n",
    "test_set_2, devtest_set_2, train_set_2, tsName_2, dtName_2, tName_2 = tts(gender_features2, allNames)\n",
    "\n",
    "test_set_3, devtest_set_3, train_set_3, tsName_3, dtName_3, tName_3 = tts(gender_features3, allNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.52007        0.684\n",
      "             3          -0.46423        0.756\n",
      "             4          -0.43018        0.774\n",
      "             5          -0.40831        0.774\n",
      "             6          -0.39348        0.777\n",
      "             7          -0.38297        0.780\n",
      "             8          -0.37525        0.779\n",
      "             9          -0.36943        0.779\n",
      "            10          -0.36493        0.780\n",
      "            11          -0.36139        0.781\n",
      "            12          -0.35857        0.781\n",
      "            13          -0.35628        0.781\n",
      "            14          -0.35441        0.781\n",
      "            15          -0.35286        0.781\n",
      "            16          -0.35157        0.781\n",
      "            17          -0.35049        0.782\n",
      "            18          -0.34957        0.782\n",
      "            19          -0.34879        0.782\n",
      "            20          -0.34812        0.782\n",
      "            21          -0.34755        0.782\n",
      "            22          -0.34705        0.782\n",
      "            23          -0.34661        0.782\n",
      "            24          -0.34623        0.782\n",
      "            25          -0.34589        0.782\n",
      "            26          -0.34560        0.782\n",
      "            27          -0.34534        0.783\n",
      "            28          -0.34510        0.783\n",
      "            29          -0.34489        0.783\n",
      "            30          -0.34471        0.782\n",
      "            31          -0.34454        0.783\n",
      "            32          -0.34438        0.783\n",
      "            33          -0.34425        0.783\n",
      "            34          -0.34412        0.783\n",
      "            35          -0.34400        0.783\n",
      "            36          -0.34390        0.783\n",
      "            37          -0.34380        0.783\n",
      "            38          -0.34372        0.783\n",
      "            39          -0.34363        0.783\n",
      "            40          -0.34356        0.783\n",
      "            41          -0.34349        0.784\n",
      "            42          -0.34342        0.784\n",
      "            43          -0.34336        0.784\n",
      "            44          -0.34331        0.784\n",
      "            45          -0.34326        0.784\n",
      "            46          -0.34321        0.784\n",
      "            47          -0.34316        0.784\n",
      "            48          -0.34312        0.784\n",
      "            49          -0.34308        0.785\n",
      "            50          -0.34304        0.785\n",
      "            51          -0.34300        0.785\n",
      "            52          -0.34297        0.785\n",
      "            53          -0.34294        0.785\n",
      "            54          -0.34291        0.784\n",
      "            55          -0.34288        0.784\n",
      "            56          -0.34285        0.784\n",
      "            57          -0.34282        0.784\n",
      "            58          -0.34280        0.784\n",
      "            59          -0.34278        0.784\n",
      "            60          -0.34275        0.784\n",
      "            61          -0.34273        0.784\n",
      "            62          -0.34271        0.784\n",
      "            63          -0.34269        0.784\n",
      "            64          -0.34267        0.785\n",
      "            65          -0.34266        0.785\n",
      "            66          -0.34264        0.785\n",
      "            67          -0.34262        0.785\n",
      "            68          -0.34261        0.785\n",
      "            69          -0.34259        0.785\n",
      "            70          -0.34258        0.785\n",
      "            71          -0.34256        0.785\n",
      "            72          -0.34255        0.785\n",
      "            73          -0.34254        0.785\n",
      "            74          -0.34252        0.785\n",
      "            75          -0.34251        0.785\n",
      "            76          -0.34250        0.785\n",
      "            77          -0.34249        0.785\n",
      "            78          -0.34248        0.785\n",
      "            79          -0.34247        0.785\n",
      "            80          -0.34246        0.785\n",
      "            81          -0.34245        0.785\n",
      "            82          -0.34244        0.785\n",
      "            83          -0.34243        0.785\n",
      "            84          -0.34242        0.785\n",
      "            85          -0.34241        0.785\n",
      "            86          -0.34240        0.785\n",
      "            87          -0.34239        0.785\n",
      "            88          -0.34239        0.785\n",
      "            89          -0.34238        0.785\n",
      "            90          -0.34237        0.785\n",
      "            91          -0.34236        0.785\n",
      "            92          -0.34236        0.785\n",
      "            93          -0.34235        0.785\n",
      "            94          -0.34234        0.785\n",
      "            95          -0.34234        0.785\n",
      "            96          -0.34233        0.785\n",
      "            97          -0.34232        0.785\n",
      "            98          -0.34232        0.785\n",
      "            99          -0.34231        0.785\n",
      "         Final          -0.34231        0.785\n",
      "0.772\n"
     ]
    }
   ],
   "source": [
    "classifier_1 = nltk.classify.MaxentClassifier.train(train_set_1)\n",
    "print(nltk.classify.accuracy(classifier_1, test_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier_1, devtest_set_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that devtest_set accuracy is 81% and test_set accuracy is 77%. Let's look how many names got wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 97\n"
     ]
    }
   ],
   "source": [
    "preds_1, errors_1 = pred_calc(dtName_1, gender_features, classifier_1)\n",
    "print('Number of errors:', len(errors_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.48597        0.730\n",
      "             3          -0.41874        0.803\n",
      "             4          -0.37981        0.814\n",
      "             5          -0.35519        0.823\n",
      "             6          -0.33839        0.826\n",
      "             7          -0.32623        0.827\n",
      "             8          -0.31705        0.829\n",
      "             9          -0.30987        0.832\n",
      "            10          -0.30410        0.834\n",
      "            11          -0.29936        0.834\n",
      "            12          -0.29539        0.835\n",
      "            13          -0.29202        0.836\n",
      "            14          -0.28912        0.836\n",
      "            15          -0.28659        0.836\n",
      "            16          -0.28437        0.836\n",
      "            17          -0.28239        0.836\n",
      "            18          -0.28062        0.836\n",
      "            19          -0.27903        0.837\n",
      "            20          -0.27759        0.837\n",
      "            21          -0.27627        0.838\n",
      "            22          -0.27507        0.838\n",
      "            23          -0.27397        0.838\n",
      "            24          -0.27295        0.838\n",
      "            25          -0.27200        0.838\n",
      "            26          -0.27113        0.838\n",
      "            27          -0.27031        0.838\n",
      "            28          -0.26955        0.838\n",
      "            29          -0.26883        0.839\n",
      "            30          -0.26816        0.839\n",
      "            31          -0.26753        0.839\n",
      "            32          -0.26693        0.839\n",
      "            33          -0.26637        0.839\n",
      "            34          -0.26584        0.839\n",
      "            35          -0.26534        0.839\n",
      "            36          -0.26486        0.839\n",
      "            37          -0.26441        0.839\n",
      "            38          -0.26398        0.838\n",
      "            39          -0.26357        0.838\n",
      "            40          -0.26317        0.838\n",
      "            41          -0.26280        0.838\n",
      "            42          -0.26244        0.838\n",
      "            43          -0.26210        0.837\n",
      "            44          -0.26177        0.837\n",
      "            45          -0.26146        0.837\n",
      "            46          -0.26116        0.838\n",
      "            47          -0.26087        0.838\n",
      "            48          -0.26059        0.838\n",
      "            49          -0.26032        0.838\n",
      "            50          -0.26007        0.838\n",
      "            51          -0.25982        0.838\n",
      "            52          -0.25958        0.838\n",
      "            53          -0.25935        0.838\n",
      "            54          -0.25913        0.838\n",
      "            55          -0.25891        0.838\n",
      "            56          -0.25870        0.838\n",
      "            57          -0.25850        0.838\n",
      "            58          -0.25831        0.838\n",
      "            59          -0.25812        0.838\n",
      "            60          -0.25794        0.838\n",
      "            61          -0.25776        0.838\n",
      "            62          -0.25759        0.838\n",
      "            63          -0.25743        0.838\n",
      "            64          -0.25726        0.838\n",
      "            65          -0.25711        0.838\n",
      "            66          -0.25696        0.838\n",
      "            67          -0.25681        0.838\n",
      "            68          -0.25667        0.838\n",
      "            69          -0.25653        0.838\n",
      "            70          -0.25639        0.838\n",
      "            71          -0.25626        0.838\n",
      "            72          -0.25613        0.838\n",
      "            73          -0.25600        0.838\n",
      "            74          -0.25588        0.838\n",
      "            75          -0.25576        0.838\n",
      "            76          -0.25565        0.838\n",
      "            77          -0.25553        0.838\n",
      "            78          -0.25542        0.838\n",
      "            79          -0.25531        0.838\n",
      "            80          -0.25521        0.838\n",
      "            81          -0.25511        0.839\n",
      "            82          -0.25501        0.839\n",
      "            83          -0.25491        0.839\n",
      "            84          -0.25481        0.838\n",
      "            85          -0.25472        0.838\n",
      "            86          -0.25463        0.838\n",
      "            87          -0.25454        0.839\n",
      "            88          -0.25445        0.839\n",
      "            89          -0.25436        0.839\n",
      "            90          -0.25428        0.839\n",
      "            91          -0.25419        0.839\n",
      "            92          -0.25411        0.839\n",
      "            93          -0.25403        0.839\n",
      "            94          -0.25396        0.839\n",
      "            95          -0.25388        0.839\n",
      "            96          -0.25381        0.839\n",
      "            97          -0.25373        0.839\n",
      "            98          -0.25366        0.839\n",
      "            99          -0.25359        0.839\n",
      "         Final          -0.25352        0.839\n",
      "0.828\n"
     ]
    }
   ],
   "source": [
    "classifier_2 = nltk.classify.MaxentClassifier.train(train_set_2)\n",
    "print(nltk.classify.accuracy(classifier_2, test_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier_2, devtest_set_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that devtest_set accuracy is 83% and test_set accuracy is 80%. Let's look how many names got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 101\n"
     ]
    }
   ],
   "source": [
    "preds_2, errors_2 = pred_calc(dtName_2, gender_features2, classifier_2)\n",
    "print('Number of errors:', len(errors_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.371\n",
      "             2          -0.52149        0.672\n",
      "             3          -0.45868        0.785\n",
      "             4          -0.41673        0.808\n",
      "             5          -0.38780        0.815\n",
      "             6          -0.36700        0.821\n",
      "             7          -0.35144        0.823\n",
      "             8          -0.33943        0.824\n",
      "             9          -0.32989        0.824\n",
      "            10          -0.32213        0.828\n",
      "            11          -0.31571        0.829\n",
      "            12          -0.31031        0.833\n",
      "            13          -0.30569        0.833\n",
      "            14          -0.30171        0.834\n",
      "            15          -0.29823        0.836\n",
      "            16          -0.29517        0.836\n",
      "            17          -0.29244        0.836\n",
      "            18          -0.29001        0.837\n",
      "            19          -0.28782        0.837\n",
      "            20          -0.28583        0.838\n",
      "            21          -0.28403        0.839\n",
      "            22          -0.28237        0.839\n",
      "            23          -0.28086        0.839\n",
      "            24          -0.27946        0.839\n",
      "            25          -0.27816        0.839\n",
      "            26          -0.27695        0.839\n",
      "            27          -0.27583        0.839\n",
      "            28          -0.27478        0.840\n",
      "            29          -0.27380        0.840\n",
      "            30          -0.27288        0.840\n",
      "            31          -0.27201        0.841\n",
      "            32          -0.27119        0.840\n",
      "            33          -0.27041        0.841\n",
      "            34          -0.26968        0.841\n",
      "            35          -0.26898        0.841\n",
      "            36          -0.26832        0.841\n",
      "            37          -0.26769        0.840\n",
      "            38          -0.26709        0.841\n",
      "            39          -0.26652        0.841\n",
      "            40          -0.26597        0.840\n",
      "            41          -0.26545        0.841\n",
      "            42          -0.26495        0.841\n",
      "            43          -0.26447        0.841\n",
      "            44          -0.26401        0.841\n",
      "            45          -0.26357        0.841\n",
      "            46          -0.26314        0.841\n",
      "            47          -0.26273        0.841\n",
      "            48          -0.26234        0.841\n",
      "            49          -0.26196        0.841\n",
      "            50          -0.26159        0.841\n",
      "            51          -0.26124        0.840\n",
      "            52          -0.26090        0.840\n",
      "            53          -0.26057        0.841\n",
      "            54          -0.26025        0.841\n",
      "            55          -0.25994        0.841\n",
      "            56          -0.25964        0.841\n",
      "            57          -0.25935        0.841\n",
      "            58          -0.25907        0.841\n",
      "            59          -0.25879        0.841\n",
      "            60          -0.25853        0.841\n",
      "            61          -0.25827        0.841\n",
      "            62          -0.25803        0.841\n",
      "            63          -0.25778        0.841\n",
      "            64          -0.25755        0.841\n",
      "            65          -0.25732        0.841\n",
      "            66          -0.25710        0.841\n",
      "            67          -0.25688        0.841\n",
      "            68          -0.25667        0.842\n",
      "            69          -0.25646        0.842\n",
      "            70          -0.25626        0.842\n",
      "            71          -0.25607        0.842\n",
      "            72          -0.25588        0.842\n",
      "            73          -0.25569        0.842\n",
      "            74          -0.25551        0.842\n",
      "            75          -0.25533        0.842\n",
      "            76          -0.25516        0.842\n",
      "            77          -0.25499        0.842\n",
      "            78          -0.25483        0.842\n",
      "            79          -0.25467        0.842\n",
      "            80          -0.25451        0.842\n",
      "            81          -0.25436        0.843\n",
      "            82          -0.25421        0.843\n",
      "            83          -0.25406        0.843\n",
      "            84          -0.25392        0.843\n",
      "            85          -0.25378        0.843\n",
      "            86          -0.25364        0.843\n",
      "            87          -0.25350        0.843\n",
      "            88          -0.25337        0.842\n",
      "            89          -0.25324        0.842\n",
      "            90          -0.25311        0.842\n",
      "            91          -0.25299        0.842\n",
      "            92          -0.25287        0.842\n",
      "            93          -0.25275        0.842\n",
      "            94          -0.25263        0.842\n",
      "            95          -0.25252        0.842\n",
      "            96          -0.25240        0.842\n",
      "            97          -0.25229        0.842\n",
      "            98          -0.25218        0.842\n",
      "            99          -0.25208        0.842\n",
      "         Final          -0.25197        0.842\n",
      "0.828\n"
     ]
    }
   ],
   "source": [
    "classifier_3 = nltk.classify.MaxentClassifier.train(train_set_3)\n",
    "print(nltk.classify.accuracy(classifier_3, test_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier_3, devtest_set_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that devtest_set accuracy is 83% and test_set accuracy is 81%. Let's look how many names got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 95\n"
     ]
    }
   ],
   "source": [
    "preds_3, errors_3 = pred_calc(dtName_3, gender_features3, classifier_3)\n",
    "print('Number of errors:', len(errors_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put all 3 features in a tablular format and see the accuract of devtest_set and test_set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>DEV_ACCURACY</th>\n",
       "      <th>TEST_ACCURACY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>First</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Second</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Final</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MODEL  DEV_ACCURACY  TEST_ACCURACY\n",
       "0   First         0.806          0.772\n",
       "1  Second         0.798          0.828\n",
       "2   Final         0.810          0.828"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([['First', nltk.classify.accuracy(classifier_1, devtest_set_1), nltk.classify.accuracy(classifier_1, test_set_1)],\n",
    "             ['Second', nltk.classify.accuracy(classifier_2, devtest_set_2), nltk.classify.accuracy(classifier_2, test_set_2)], \n",
    "             ['Final', nltk.classify.accuracy(classifier_3, devtest_set_3), nltk.classify.accuracy(classifier_3, test_set_3)]],\n",
    "            columns = ['MODEL', 'DEV_ACCURACY', 'TEST_ACCURACY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our Max Entropy classifier uses an iterative method to maximize the performance of the training corpus classification. In this case the default number of iteration was 100. Due to this  it takes a long time to train a huge dataset and could also explain why it is not as popular.  One strange behavior we observed with this classifier is devtest_set having lower accuracy than test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_table(allNames, tsPred):\n",
    "    tag = [name for name in tsPred if [name for (name, tag) in allNames]]\n",
    "    perform = []\n",
    "    for i in tsPred:\n",
    "        if (i[0] == 'male') & (i[1] == 'male'):\n",
    "            perform.append('correct male')\n",
    "        elif (i[0] == 'female') & (i[1] == 'female'):\n",
    "            perform.append('correct female')\n",
    "        elif (i[0] == 'male') & (i[1] == 'female'):\n",
    "            perform.append('incorrect male')\n",
    "        else:\n",
    "            perform.append('incorrect female')\n",
    "    correct_male = perform.count('correct male')\n",
    "    correct_female = perform.count('correct female')\n",
    "    incorrect_female = perform.count('incorrect female')\n",
    "    incorrect_male = perform.count('incorrect male')\n",
    "    \n",
    "    performance_table_pct = pd.DataFrame([['Females', \"{:.0%}\".format(correct_female / (correct_female + incorrect_female)), \"{:.0%}\".format(incorrect_female / (correct_female + incorrect_female))],\n",
    "             ['Males', \"{:.0%}\".format(correct_male / (correct_male + incorrect_male)), \"{:.0%}\".format(incorrect_male / (correct_male + incorrect_male))]],\n",
    "            columns = ['Gender', 'Percent Correct', 'Percent Incorrect'])\n",
    "    performance_table_pct.style.hide_index()\n",
    "    \n",
    "    return performance_table_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wanted to take a look at the relative accuracies with respect to the two genders. The table below breaks down the results of our final model. It shows that our model predicted female names with a greater accuracy than male names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Percent Correct</th>\n",
       "      <th>Percent Incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Females</td>\n",
       "      <td>83%</td>\n",
       "      <td>17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Males</td>\n",
       "      <td>76%</td>\n",
       "      <td>24%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender Percent Correct Percent Incorrect\n",
       "0  Females             83%               17%\n",
       "1    Males             76%               24%"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_table_pct = summ_table(allNames, tsPred)\n",
    "performance_table_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the better accuracy for female names, we remembered that the dataset is skewed fairly heavily in favor of female names (63% female / 37% male). In order to see how much the greater accuracy for female names was driven by the disbalance within the dataset we decided to try two basic approaches of dealing with an imbalanced dataset: Undersampling and Oversampling. We decided to re-evaluate our model after adjusting the training data to be balanced - once by removing the extra female names and once by copying in repeats of male names to balance out the number of female names. \n",
    "\n",
    "#### Effect of undersampling the female set within the training data\n",
    "\n",
    "We wrote a function that took both the training dataset [train_set] and the list of names associated within the training dataset [tName], evaluated which gender's names there were more of and performed either an undersampling of the greater set or an oversampling of the smaller (based on input from the user). We then applied this function to the two lists before running our model again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'over' input below signifies whether the user wants to undersample or oversample the dataset (default undersample)\n",
    "def balance_train(train_set, tName, under = 1):\n",
    "    gender = []\n",
    "    for name,g in tName:\n",
    "        if g == \"female\":\n",
    "            gender.append(1)\n",
    "        else:\n",
    "            gender.append(0)\n",
    "    n_female = sum(gender)\n",
    "    n_male = len(gender) - n_female\n",
    "    if n_female == n_male:\n",
    "        return train_set, tName\n",
    "    elif n_female > n_male:\n",
    "        more = \"F\"\n",
    "        delta = n_female - n_male\n",
    "    else:\n",
    "        more = \"M\"\n",
    "        delta = n_male - n_female\n",
    "    \n",
    "    idx_males = []\n",
    "    idx_males = [i for i, val in enumerate(tName) if val[1] == \"male\"]\n",
    "    idx_females = []\n",
    "    idx_females = [i for i, val in enumerate(tName) if val[1] == \"female\"]\n",
    "    \n",
    "    remove = []\n",
    "    copy = []\n",
    "    if more == \"F\":\n",
    "        remove = idx_females\n",
    "        remove = remove[-delta:]\n",
    "        copy = idx_males\n",
    "    elif more == \"M\":\n",
    "        remove = idx_males\n",
    "        remove = remove[-delta:]\n",
    "        copy = idx_females\n",
    "    \n",
    "    if under == 1:\n",
    "        for index in reversed(remove):\n",
    "            del tName[index]\n",
    "            del train_set[index]\n",
    "    elif under == 0:\n",
    "        for i in range(0,delta):\n",
    "            tName.append(tName[copy[i]])\n",
    "            train_set.append(train_set[copy[i]])\n",
    "    return train_set, tName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.796\n",
      "Number of errors: 102\n"
     ]
    }
   ],
   "source": [
    "test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features3, allNames)\n",
    "\n",
    "train_set, tName = balance_train(train_set, tName,1)\n",
    "\n",
    "nbClass4 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: ', nltk.classify.accuracy(nbClass4, devtest_set))\n",
    "\n",
    "preds4, errors4 = pred_calc(dtName, gender_features3, nbClass4)\n",
    "print('Number of errors:', len(errors4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Percent Correct</th>\n",
       "      <th>Percent Incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Females</td>\n",
       "      <td>78%</td>\n",
       "      <td>22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Males</td>\n",
       "      <td>81%</td>\n",
       "      <td>19%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender Percent Correct Percent Incorrect\n",
       "0  Females             78%               22%\n",
       "1    Males             81%               19%"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsPred4, tsError4 = pred_calc(tsName, gender_features3,nbClass4)\n",
    "performance_table_pct = summ_table(allNames, tsPred4)\n",
    "performance_table_pct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of oversampling the male set within the training data\n",
    "For the oversampling of male data we wrote a similar function that evaluated how many fewer male names there were and appended that many copies of names from the lists themselves to even the numbers out. We again applied this function and re-ran the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.798\n",
      "Number of errors: 101\n"
     ]
    }
   ],
   "source": [
    "test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features3, allNames)\n",
    "\n",
    "train_set, tName = balance_train(train_set, tName, 0)\n",
    "\n",
    "nbClass5 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print('Accuracy: ', nltk.classify.accuracy(nbClass5, devtest_set))\n",
    "\n",
    "preds5, errors5 = pred_calc(dtName, gender_features3, nbClass5)\n",
    "print('Number of errors:', len(errors5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Percent Correct</th>\n",
       "      <th>Percent Incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Females</td>\n",
       "      <td>78%</td>\n",
       "      <td>22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Males</td>\n",
       "      <td>82%</td>\n",
       "      <td>18%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender Percent Correct Percent Incorrect\n",
       "0  Females             78%               22%\n",
       "1    Males             82%               18%"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsPred5, tsError5 = pred_calc(tsName, gender_features3,nbClass5)\n",
    "summ_table(allNames, tsPred5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the summary tables, both approaches to balancing the training dataset reduced the accuracy of predicting female names, but increased that of predicting male names. This is driven by the fact that the algorithm has fewer female names to determine patterns from, leading to less accurate predictions. This in turn increases the relative impact of the male-predictor patterns. \n",
    "\n",
    "#### Impact of original random_seed to split the data\n",
    "\n",
    "The names that make their way into the training set will obviously have an impact on how accurate a predictor model is. Below are a few result tables showing the difference in accuracy based on different initial train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_correct(allNames, tsPred):\n",
    "    tag = [name for name in tsPred if [name for (name, tag) in allNames]]\n",
    "    perform = []\n",
    "    for i in tsPred:\n",
    "        if (i[0] == 'male') & (i[1] == 'male'):\n",
    "            perform.append('correct male')\n",
    "        elif (i[0] == 'female') & (i[1] == 'female'):\n",
    "            perform.append('correct female')\n",
    "        elif (i[0] == 'male') & (i[1] == 'female'):\n",
    "            perform.append('incorrect male')\n",
    "        else:\n",
    "            perform.append('incorrect female')\n",
    "            \n",
    "    correct_male = perform.count('correct male')\n",
    "    correct_female = perform.count('correct female')\n",
    "    incorrect_female = perform.count('incorrect female')\n",
    "    incorrect_male = perform.count('incorrect male')\n",
    "    \n",
    "    female_pct_corr = correct_female / (correct_female + incorrect_female)\n",
    "    male_pct_corr = correct_male / (correct_male + incorrect_male)\n",
    "    \n",
    "    return female_pct_corr , male_pct_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Normal</th>        <th class=\"col_heading level0 col1\" >Undersampled</th>        <th class=\"col_heading level0 col2\" >Oversampled</th>    </tr>    <tr>        <th class=\"index_name level0\" >Seed</th>        <th class=\"index_name level1\" >Gender</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level0_row0\" class=\"row_heading level0 row0\" rowspan=2>865</th>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row0\" class=\"row_heading level1 row0\" >F</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row0_col0\" class=\"data row0 col0\" >81.9%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row0_col1\" class=\"data row0 col1\" >76.8%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row0_col2\" class=\"data row0 col2\" >76.8%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row1\" class=\"row_heading level1 row1\" >M</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row1_col0\" class=\"data row1 col0\" >80.0%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row1_col1\" class=\"data row1 col1\" >84.9%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row1_col2\" class=\"data row1 col2\" >84.9%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level0_row2\" class=\"row_heading level0 row2\" rowspan=2>888</th>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row2\" class=\"row_heading level1 row2\" >F</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row2_col0\" class=\"data row2 col0\" >80.1%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row2_col1\" class=\"data row2 col1\" >75.5%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row2_col2\" class=\"data row2 col2\" >75.5%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row3\" class=\"row_heading level1 row3\" >M</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row3_col0\" class=\"data row3 col0\" >78.6%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row3_col1\" class=\"data row3 col1\" >81.5%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row3_col2\" class=\"data row3 col2\" >81.5%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level0_row4\" class=\"row_heading level0 row4\" rowspan=2>616</th>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row4\" class=\"row_heading level1 row4\" >F</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row4_col0\" class=\"data row4 col0\" >79.6%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row4_col1\" class=\"data row4 col1\" >75.2%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row4_col2\" class=\"data row4 col2\" >75.2%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row5\" class=\"row_heading level1 row5\" >M</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row5_col0\" class=\"data row5 col0\" >78.5%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row5_col1\" class=\"data row5 col1\" >80.1%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row5_col2\" class=\"data row5 col2\" >80.1%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level0_row6\" class=\"row_heading level0 row6\" rowspan=2>171</th>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row6\" class=\"row_heading level1 row6\" >F</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row6_col0\" class=\"data row6 col0\" >82.9%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row6_col1\" class=\"data row6 col1\" >80.3%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row6_col2\" class=\"data row6 col2\" >80.3%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row7\" class=\"row_heading level1 row7\" >M</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row7_col0\" class=\"data row7 col0\" >74.2%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row7_col1\" class=\"data row7 col1\" >78.4%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row7_col2\" class=\"data row7 col2\" >78.4%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level0_row8\" class=\"row_heading level0 row8\" rowspan=2>163</th>\n",
       "                        <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row8\" class=\"row_heading level1 row8\" >F</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row8_col0\" class=\"data row8 col0\" >83.3%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row8_col1\" class=\"data row8 col1\" >80.9%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row8_col2\" class=\"data row8 col2\" >80.9%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122level1_row9\" class=\"row_heading level1 row9\" >M</th>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row9_col0\" class=\"data row9 col0\" >80.7%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row9_col1\" class=\"data row9 col1\" >83.0%</td>\n",
       "                        <td id=\"T_2d93bc0e_be1f_11ea_a86d_acde48001122row9_col2\" class=\"data row9 col2\" >83.0%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a1ac983d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seeds = 5\n",
    "seeds = random.sample(range(0,1000),n_seeds)\n",
    "iterables = [seeds,['F','M']]\n",
    "index = pd.MultiIndex.from_product(iterables, names = ['Seed','Gender'])\n",
    "df = pd.DataFrame(np.zeros((n_seeds*2, 3)),index =index, columns = [\"Normal\",\"Undersampled\",\"Oversampled\"])\n",
    "counter = 0\n",
    "for seed in seeds:\n",
    "    random.seed(seed)\n",
    "    allNames = males + females\n",
    "    random.shuffle(allNames)\n",
    "    test_set, devtest_set, train_set, tsName, dtName, tName = tts(gender_features3, allNames)\n",
    "    for i in range(0,3):\n",
    "        train_set2 = []\n",
    "        tname = []\n",
    "        if i == 0:\n",
    "            train_set2 = train_set\n",
    "        elif i == 1:\n",
    "            train_set2, tName = balance_train(train_set, tName, 1)\n",
    "        elif i == 2:\n",
    "            train_set2, tName = balance_train(train_set, tName, 0)\n",
    "            \n",
    "        nbClass = nltk.NaiveBayesClassifier.train(train_set2)\n",
    "        tsPred, tsError = pred_calc(tsName, gender_features3,nbClass)\n",
    "        f_pct, m_pct = pull_correct(allNames, tsPred)\n",
    "        df.iloc[counter][i] = f_pct\n",
    "        df.iloc[counter+1][i] = m_pct\n",
    "    allNames =[]\n",
    "    counter = counter + 2\n",
    "    \n",
    "df.style.format({\n",
    "    'Normal': '{:,.1%}'.format,\n",
    "    'Undersampled': '{:,.1%}'.format,\n",
    "    'Oversampled': '{:,.1%}'.format,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above our female name accuracy changes significantly even just for the Normal training set (ranging anywhere from below 80% to above 85%) depending on the random seed chosen to split the data into train and test datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we created a Naive Bayes Classifier to predict the gender of a given name from our Names corpus. With a robust corpus of 7944 names, we randomized and split the data into a test set of 500 names (for final testing of our model), a development test set of 500 names to utilize while tweaking and adjusting our model features, and a training set that we used throughout to train our model -- which contained 6944 names.\n",
    "\n",
    "##### Identifying the most informative features and creating our classifier\n",
    "\n",
    "After several runs of our Naive Bayes Classifier, we were able to pinpoint some of the most informative features:\n",
    "\n",
    "+ the value of the first and last few letters of a given name\n",
    "+ the number of vowels present in a given name\n",
    "+ the presence of double letters (i.e. \"ee\", \"oo\", etc.) in a given name\n",
    "+ the length of a given name\n",
    "+ the number of consonants in a given name\n",
    "+ and the presence of certain letters in a given name (based on research, some letters seemed to be present more frequently in male or female names)\n",
    "\n",
    "With our features identified, we were able to use our final classifier on our test set of 500 names. We found that our classifier, and these features were able to successfully predict the gender of about 80% of the names in our test corpus. \n",
    "\n",
    "##### Evaluating the performance of our classifier and resampling our training dataset\n",
    "\n",
    "Although this was interesting, we did realize that there was an unequal distribution of male and female names present in our Names corpus, and thus found that the prediction accuracy was slightly higher for females than for males in our final evaluation. We can attribute this to the fact that the model had more female names to train on, which ultimately led to a slightly better performance when we subjected it to our final test set.\n",
    "\n",
    "To investigate this further, we decided to retrain our model in two ways: \n",
    "\n",
    "1) We undersampled the female names in our training set in order to create a more equal proportion of female/male names for our classifier to determine patterns from -- to do this, we randomly removed about 2000 female names from our training data\n",
    "\n",
    "2) We oversampled the male names in our training set in order to match the number of female names -- to do this, we randomly copied about 2000 male names to match the number of female names present in the training data\n",
    "\n",
    "\n",
    "##### Summary of our findings\n",
    "\n",
    "After training our data with these new splits, we then subjected this new classifier on our test data and found that the overall performance decreased slightly (by about 1%). As we can see from the summary tables, both approaches to balancing the training dataset reduced the accuracy of predicting female names, but increased that of predicting male names. This was to be expected since the resampled training data and the corresponding algorithm had fewer female names to determine patterns from, leading to less accurate predictions. However, this in turn increased the relative impact of the male-predictor patterns.\n",
    "\n",
    "Overall, we were able to implement a Naive Bayes Classifier and Maximum Entropy Classifier on our names corpus, and after conducting a few different sampling techniques, we generated a classifier that preformed quite well (~80% accuracy predicting female and male names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. D. M. Sidhu and P. M. Pexman. What’s in a name? sound symbolism and gender in first names. PLOS ONE, 10(5):e0126809, 2015.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
